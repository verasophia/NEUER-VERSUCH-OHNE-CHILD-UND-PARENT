---
title:  | 
        | Klasse im Puls
        | Model statement
author: 
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    fig_caption: yes
    theme: spacelab 
    highlight: pygments
    toc: TRUE
    toc_depth: 2
    number_sections: TRUE
    toc_float:
      smooth_scroll: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment=NA)
rm(list=ls()) # rm() needs to be specified; using list removes a bunch of objects; ls() lists all objects in the current workspace --> command removes all objects from R memory
library(tidyverse)
library(plm)       # Panel data analysis library
library(stargazer)
library(effects)
library(ggplot2)
library(brms)
library(ggdist)
library(Hmisc)
library(mosaic)
library(foreign)
library(lme4)
library(rstanarm)
set.seed(132435)
```


The purpose of this vignette is to develop and test a model framework for analyzing experiments in school classes.


# Data generating process 


Our aim is to estimate the effect of a classroom experiment. We consider one treatment group and one control group.


```{r}
class.size <- 30 # number of students in each class
student <- seq(1:class.size)
class <- seq(1:12)

wave <- seq(1:5)

d <- expand.grid(wave = wave, student = student, class = class)

rm(wave, class, student)
```

First, we define the data structure for `r max(d$class)` groups (classrooms), `r class.size` students per class, and `r max(d$wave)+1` measurement occasions.

```{r}



d$treated <- ifelse(d$class %% 2 == 1, 1, 0)

# create treated-wave interactions: "tXw"
d$tXw2 <- ifelse( (d$treated==1 & d$wave==2), 1, 0)
d$tXw3 <- ifelse( (d$treated==1 & d$wave==3), 1, 0)
d$tXw4 <- ifelse( (d$treated==1 & d$wave==4), 1, 0)
d$tXw5 <- ifelse( (d$treated==1 & d$wave==5), 1, 0)
  
  # ifelse(d$class==1 | d$class==3 | d$class==5 | d$class==7 | d$class==9 | d$class==11, 1, 0)


d$school <- ceiling(d$class / 2)


# d$school <- ifelse(d$class==1 | d$class==2, 1, ifelse(d$class==3 | d$class==4, 2, ifelse(d$class==5 | d$class==6, 3, ifelse(d$class==7 | d$class==8, 4, ifelse(d$class==9 | d$class==10, 5, ifelse(d$class==11 | d$class==12, 6,0))))))

df <- unique(d[c("student", "class")]) # creates new data sheet with those two variables from first data sheet
df <- df %>% mutate(ustudent = 1:nrow(df)) %>%
#mutate() adds new variables and preserves existing ones
    select(ustudent, everything()) #student ID für jeden einzelnen Schüler unabhängig von der Klasse
df$student.effect <- rnorm(nrow(df), mean=0, sd=1)
d <- left_join(d, df, by=c("class", "student"))
df <- unique(df <- unique(d[c("school")]))
df$school.effect <- rnorm(nrow(df), mean=0, sd=1.5)
d <- left_join(d, df, by="school")
tail(d)

```

The outcomes are determined by the following data generating process.

```{r}
teffect2 <- 0.6 # treatment effect wave 2
teffect3 <- 0.8 # treatment effect wave 3
teffect4 <- 1 # treatment effect wave 4
teffect5 <- 1.2 # treatment effect wave 5
wave.effect <- -1 # wave effects
class.effect <- 2
cons <- 0
d$y <- cons + class.effect*d$treated + 
              wave.effect*d$wave +
              teffect2*d$tXw2 +
              teffect3*d$tXw3 +
              teffect4*d$tXw4 +
              teffect5*d$tXw5 +
              d$student.effect +
              d$school.effect +
              rnorm(nrow(d), mean=0, sd=1)
saveRDS(d, file = "../data/simulation.RDS")

head(d)
schule2 <- d %>% filter(school==2) %>% 
  select(wave, student, class, treated, tXw2, tXw3, tXw4, tXw5, school, ustudent, student.effect, school.effect, y)
```

# BRMS
## Varying effects and the underfitting/overfittin trade-off
Varying intercepts are just regularized estimates, but adaptively regularized by estimating how diverse the clusters are while estimating the features of each cluster. This fact is not easy to grasp... A major benefit of using varying effects estimates, instead of the empirical raw estimates, is that they provide more accurate estimates of the individual cluster (school) intercepts. On average, the verying effects actually provid a better estimate of the individual school (cluster) means. The reson that the varying intercepts provide better estimates is that they do a better job of trading off underfitting and overfitting (9408).

In this section, we explicate this by contrasting three persepectives:
* complete pooling (i.e. a singel-$\alpha$ model)
* no pooling (i.e., the single-level $\alpha_{tank[i]}$ model), and
* partial pooling [i.e., the multilevel model for which $\alpha_j \sim {\sf Normal}(\overline{\alpha}, \sigma)$].

To demonstrate the [the magic of the multilevel model], we'll simulate some *tadpole* data. That way, we'll know the true *per-pond survival* probabilities. Then we can compare the no-pooling estimates to the partial pooling estimates, by computing how close each gets to the true values they are trying to estimate. The rest of this section shows how to do such a simulation (p409).

### The model.

The simulation formula should look familiar.
${\sf surv}_i \sim {\sf Binomial}(n_i,p_i)$
${\sf logit}(p_i) = \alpha_{{\sf pond}[i]}}$
$\alpha_j \sim {\sf Normal }(\overline{\alpha}, \sigma)$
$\overline{\alpha} \sim {\sf Normal}(0,1.5)$
$\sigma \sim {\sf Exponential}(1)$

### Assign values to the parameters.

Here we follow along with McElreath and ""assign specific values representative of the actual *tadpole data* p409. Because he includes a `set.seed()` line, our results should match his exactly.

```{r}
a_bar <- 1.5
sigma <- 1.5
n_ponds <- 60

set.seed(5005)

dsim <- 
  tibble(pond   = 1:n_ponds,
         ni     = rep(c(5, 10, 25, 35), each = n_ponds / 4) %>% as.integer(),
         true_a = rnorm(n = n_ponds, mean = a_bar, sd = sigma))

head(dsim)
```
McElreath twice urged us to inspect the contents of this simulation. In addition to looking at the data with `head()`, we might as well plot.

```{r}
dsim %>% 
  mutate(ni = factor(ni)) %>% 
  
  ggplot(aes(x = true_a, y = ni)) +
  stat_dotsinterval(fill = "orange2", slab_size = 0, .width = .5) +
  ggtitle("Log-odds varying by # tadpoles per pond") +
  theme(plot.title = element_text(size = 14))
```
### Sumulate survivors.
Each *pond* $i$ has $n_i$ potential *survivors*, and nature flips each tadpoles's coin, so to speak, with probability *of survival* $p_i$. This probability $p_i$ is implied by the model definition, and is equal to:

$p_i = \frac{{\sf exp}(\alpha_i)}{1+{\sf exp}(\alpha_i)}$

The model uses a logit link, and so the probability is defined by the [`inv_logit_scaled()`] function p411.

Although McElreath shared his `set.seed()` number in the last section, he didn't share it for this bit. We'll go ahead and carr over the one from last time. Hoever, in a moment we'll see this clearly wasn't the one he uses here. As a consequence, our results will deviate a bit from his.

```{r}
set.seed(5005)
(
  dsim <-
  dsim %>%
  mutate(si = rbinom(n = n(), prob = inv_logit_scaled(true_a), size = ni))
)
```
### Compute the no-pooling estimates.
The no-pooling estimates (i.e., $\alpha_{{\sf tank}_i}$) are the results of simple algebra.

```{r}
(
  dsim <-
  dsim %>%
  mutate(p_nopool = si / ni)
)
```
"These are the same no-pooling estimates you'd get by fittin a model with a dummy variable for each *pond* and flat priors induce no regularization" p411. That is, these are the same kinds of estimates we got back when we fit `b13.1`.

### Compute the partial pooling estimates.
Fit the multilevel (partial-pooling) model.

```{r}
b13.3 <- 
  brm(data = dsim, 
      family = binomial,
      si | trials(ni) ~ 1 + (1 | pond),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(exponential(1), class = sd)),
      iter = 200, warmup = 100, chains = 4, cores = 4,
      seed = 13,
      # file = "fits/b13.03"
  )
```
Here's our standard **brms** summary
```{r}
print(b13.3)
```
I'm not aware that you can use McElreath's `depth=2` trick in **brms** for `Summary()` or `print`. However, you can get most of that information and more with the Stan-like summary using the $fit syntax

```{r}
b13.3$fit
```
As an aside, notice how this summary still reports the old-style `n_eff` values, rather than the updated `Bulk_ESS` and `Trail_ESS` values. I suspect this will change sometime soon.

Let's get ready for the diagnostic plot of Figure 13.3. First we add the partially pooled estimates, as summarized by their posterior means, to the `dsim` data. Then we compute error values.

```{r}
# we could have included this step in the clock of code below, if we wanted
p_partpool <- 
  coef(b13.3)$pond[, , ] %>% 
  data.frame() %>%
  transmute(p_partpool = inv_logit_scaled(Estimate))

dsim <- 
  dsim %>%
  bind_cols(p_partpool) %>% 
  mutate(p_true = inv_logit_scaled(true_a)) %>%
  mutate(nopool_error   = abs(p_nopool   - p_true),
         partpool_error = abs(p_partpool - p_true))

dsim %>% 
  glimpse()
```
Here is our code for Figure 13.3. The extra data processing for `dfline` is how we get the values necessary for the horizontal summary lines.

```{r}
dfline <- 
  dsim %>%
  select(ni, nopool_error:partpool_error) %>%
  pivot_longer(-ni) %>%
  group_by(name, ni) %>%
  summarise(mean_error = mean(value)) %>%
  mutate(x    = c( 1, 16, 31, 46),
         xend = c(15, 30, 45, 60))
  
dsim %>% 
  ggplot(aes(x = pond)) +
  geom_vline(xintercept = c(15.5, 30.5, 45.4), 
             color = "white", size = 2/3) +
  geom_point(aes(y = nopool_error), color = "orange2") +
  geom_point(aes(y = partpool_error), shape = 1) +
  geom_segment(data = dfline, 
               aes(x = x, xend = xend, 
                   y = mean_error, yend = mean_error),
               color = rep(c("orange2", "black"), each = 4),
               linetype = rep(1:2, each = 4)) +
  annotate(geom = "text", 
           x = c(15 - 7.5, 30 - 7.5, 45 - 7.5, 60 - 7.5), y = .45, 
           label = c("tiny (5)", "small (10)", "medium (25)", "large (35)")) +
  scale_x_continuous(breaks = c(1, 10, 20, 30, 40, 50, 60)) +
  labs(title = "Estimate error by model type",
       subtitle = "The horizontal axis displays pond number. The vertical axis measures\nthe absolute error in the predicted proportion of survivors, compared to\nthe true value used in the simulation. The higher the point, the worse\nthe estimate. No-pooling shown in orange. Partial pooling shown in black.\nThe orange and dashed black lines show the average error for each kind\nof estimate, across each initial density of tadpoles (pond size).",
       y = "absolute error") +
  theme(panel.grid.major = element_blank(),
        plot.subtitle = element_text(size = 10))
```
If you wanted to quantify the differences in simple summaries, you might execute something like this.
```{r}
dsim %>%
  select(ni, nopool_error:partpool_error) %>%
  pivot_longer(-ni) %>%
  group_by(name) %>%
  summarise(mean_error   = mean(value) %>% round(digits = 3),
            median_error = median(value) %>% round(digits = 3))
```
Although many years of work in statistic have shown that partially pooled estimates are better, on average, this is not always the case. Our results are an example of this. Mc Elreath addressed this directly:

*But there are some cases in which the no-pooling estimates are better. These exceptions often result from ponds with extreme probabilities of survival. The partial pooling estimates shrink such extreme ponds towards the mean, because few ponds exhibit such extreme behavior. But sometimes outliers really are outliers. p414*

I originally learned about the multilevel in order to work with longitudinal data. In that context, I found the basic principles of a multilevel structure quite intuitive. The concept of partial pooling, however, wook me some time to wap my head around. If you're sturggling with this, be patient and keep chipping away.

When McElreath lectured on this topic in 2015, he traced partial pooling to statistician Charles M. Stein. Efron and Morris (1977) wrote the now classic paper, *Stein's paradaox* in statistics, whcih does a nice job breaking down why partial pooling can be so powerful. One of the primary examples they used in the paper was of 1970 batting average data. If you'd like more practice seeing how partial pooling works -- or if you just like baseball --, check ou my blog post, *Stein's paradox and what partial pooling can do for you*.

# With my data:


```{r}
d.p <- pdata.frame(d, index = c("ustudent", "wave"))
```
```{r}
m0 <- plm(y ~ wave + treated + tXw2 + tXw3 + tXw4 + tXw5 + student.effect + factor(school), data=d.p, model = "random")
saveRDS(m0, file = "../results/simulation/no_pooling_sim.RDS")
m0 <- readRDS("../results/simulation/no_pooling_sim.RDS")
summary(m0)
```


```{r}
# school 1
wave2m0.sch1 <- summary(m0)$coefficient["tXw2",1]
wave3m0.sch1 <- summary(m0)$coefficient["tXw3",1]
wave4m0.sch1 <- summary(m0)$coefficient["tXw4",1]
wave5m0.sch1 <- summary(m0)$coefficient["tXw5",1]
wave2m0.sch1.se <- summary(m0)$coefficient["tXw2",2]
wave3m0.sch1.se <- summary(m0)$coefficient["tXw3",2]
wave4m0.sch1.se <- summary(m0)$coefficient["tXw4",2]
wave5m0.sch1.se <- summary(m0)$coefficient["tXw5",2]
m0.sch1.df <- data.frame(school = c(1, 1, 1, 1),
                         wave = seq(2:5),
                         estimate = c(wave2m0.sch1, wave3m0.sch1, wave4m0.sch1, wave5m0.sch1),
                         lower = c(wave2m0.sch1 - wave2m0.sch1.se,
                                   wave3m0.sch1 - wave3m0.sch1.se,
                                   wave4m0.sch1 - wave4m0.sch1.se,
                                   wave5m0.sch1 - wave5m0.sch1.se),
                         upper = c(wave2m0.sch1 + wave2m0.sch1.se,
                                   wave3m0.sch1 + wave3m0.sch1.se,
                                   wave4m0.sch1 + wave4m0.sch1.se,
                                   wave5m0.sch1 + wave5m0.sch1.se))
m0.sch1.df

names(d)[names(d) == 'school'] <- 'school'

# school 2
wave2m0.sch2 <- summary(m0)$coefficient["tXw2",1] + summary(m0)$coefficient["factor(school)2",1]
wave3m0.sch2 <- summary(m0)$coefficient["tXw3",1] + summary(m0)$coefficient["factor(school)2",1]
wave4m0.sch2 <- summary(m0)$coefficient["tXw4",1] + summary(m0)$coefficient["factor(school)2",1]
wave5m0.sch2 <- summary(m0)$coefficient["tXw5",1] + summary(m0)$coefficient["factor(school)2",1]
wave2m0.sch2.se <- summary(m0)$coefficient["tXw2",2] + summary(m0)$coefficient["factor(school)2",2]
wave3m0.sch2.se <- summary(m0)$coefficient["tXw3",2] + summary(m0)$coefficient["factor(school)2",2]
wave4m0.sch2.se <- summary(m0)$coefficient["tXw4",2] + summary(m0)$coefficient["factor(school)2",2]
wave5m0.sch2.se <- summary(m0)$coefficient["tXw5",2] + summary(m0)$coefficient["factor(school)2",2]
m0.sch2.df <- data.frame(school = c(2, 2, 2, 2),
                         wave = seq(2:5),
                         estimate = c(wave2m0.sch2, wave3m0.sch2, wave4m0.sch2, wave5m0.sch2),
                         lower = c(wave2m0.sch2 - wave2m0.sch2.se,
                                   wave3m0.sch2 - wave3m0.sch2.se,
                                   wave4m0.sch2 - wave4m0.sch2.se,
                                   wave5m0.sch2 - wave5m0.sch2.se),
                         upper = c(wave2m0.sch2 + wave2m0.sch2.se,
                                   wave3m0.sch2 + wave3m0.sch2.se,
                                   wave4m0.sch2 + wave4m0.sch2.se,
                                   wave5m0.sch2 + wave5m0.sch2.se))
m0.sch2.df

# school 3
wave2m0.sch3 <- summary(m0)$coefficient["tXw2",1] + summary(m0)$coefficient["factor(school)3",1]
wave3m0.sch3 <- summary(m0)$coefficient["tXw3",1] + summary(m0)$coefficient["factor(school)3",1]
wave4m0.sch3 <- summary(m0)$coefficient["tXw4",1] + summary(m0)$coefficient["factor(school)3",1]
wave5m0.sch3 <- summary(m0)$coefficient["tXw5",1] + summary(m0)$coefficient["factor(school)3",1]
wave2m0.sch3.se <- summary(m0)$coefficient["tXw2",2] + summary(m0)$coefficient["factor(school)3",2]
wave3m0.sch3.se <- summary(m0)$coefficient["tXw3",2] + summary(m0)$coefficient["factor(school)3",2]
wave4m0.sch3.se <- summary(m0)$coefficient["tXw4",2] + summary(m0)$coefficient["factor(school)3",2]
wave5m0.sch3.se <- summary(m0)$coefficient["tXw5",2] + summary(m0)$coefficient["factor(school)3",2]
m0.sch3.df <- data.frame(school = c(3, 3, 3, 3),
                         wave = seq(2:5),
                         estimate = c(wave2m0.sch3, wave3m0.sch3, wave4m0.sch3, wave5m0.sch3),
                         lower = c(wave2m0.sch3 - wave2m0.sch3.se,
                                   wave3m0.sch3 - wave3m0.sch3.se,
                                   wave4m0.sch3 - wave4m0.sch3.se,
                                   wave5m0.sch3 - wave5m0.sch3.se),
                         upper = c(wave2m0.sch3 + wave2m0.sch3.se,
                                   wave3m0.sch3 + wave3m0.sch3.se,
                                   wave4m0.sch3 + wave4m0.sch3.se,
                                   wave5m0.sch3 + wave5m0.sch3.se))
m0.sch3.df

# school 4
wave2m0.sch4 <- summary(m0)$coefficient["tXw2",1] + summary(m0)$coefficient["factor(school)4",1]
wave3m0.sch4 <- summary(m0)$coefficient["tXw3",1] + summary(m0)$coefficient["factor(school)4",1]
wave4m0.sch4 <- summary(m0)$coefficient["tXw4",1] + summary(m0)$coefficient["factor(school)4",1]
wave5m0.sch4 <- summary(m0)$coefficient["tXw5",1] + summary(m0)$coefficient["factor(school)4",1]
wave2m0.sch4.se <- summary(m0)$coefficient["tXw2",2] + summary(m0)$coefficient["factor(school)4",2]
wave3m0.sch4.se <- summary(m0)$coefficient["tXw3",2] + summary(m0)$coefficient["factor(school)4",2]
wave4m0.sch4.se <- summary(m0)$coefficient["tXw4",2] + summary(m0)$coefficient["factor(school)4",2]
wave5m0.sch4.se <- summary(m0)$coefficient["tXw5",2] + summary(m0)$coefficient["factor(school)4",2]
m0.sch4.df <- data.frame(school = c(4, 4, 4, 4),
                         wave = seq(2:5),
                         estimate = c(wave2m0.sch4, wave3m0.sch4, wave4m0.sch4, wave5m0.sch4),
                         lower = c(wave2m0.sch4 - wave2m0.sch4.se,
                                   wave3m0.sch4 - wave3m0.sch4.se,
                                   wave4m0.sch4 - wave4m0.sch4.se,
                                   wave5m0.sch4 - wave5m0.sch4.se),
                         upper = c(wave2m0.sch4 + wave2m0.sch4.se,
                                   wave3m0.sch4 + wave3m0.sch4.se,
                                   wave4m0.sch4 + wave4m0.sch4.se,
                                   wave5m0.sch4 + wave5m0.sch4.se))
m0.sch4.df

# school 5
wave2m0.sch5 <- summary(m0)$coefficient["tXw2",1] + summary(m0)$coefficient["factor(school)5",1]
wave3m0.sch5 <- summary(m0)$coefficient["tXw3",1] + summary(m0)$coefficient["factor(school)5",1]
wave4m0.sch5 <- summary(m0)$coefficient["tXw4",1] + summary(m0)$coefficient["factor(school)5",1]
wave5m0.sch5 <- summary(m0)$coefficient["tXw5",1] + summary(m0)$coefficient["factor(school)5",1]
wave2m0.sch5.se <- summary(m0)$coefficient["tXw2",2] + summary(m0)$coefficient["factor(school)5",2]
wave3m0.sch5.se <- summary(m0)$coefficient["tXw3",2] + summary(m0)$coefficient["factor(school)5",2]
wave4m0.sch5.se <- summary(m0)$coefficient["tXw4",2] + summary(m0)$coefficient["factor(school)5",2]
wave5m0.sch5.se <- summary(m0)$coefficient["tXw5",2] + summary(m0)$coefficient["factor(school)5",2]
m0.sch5.df <- data.frame(school = c(5, 5, 5, 5),
                         wave = seq(2:5),
                         estimate = c(wave2m0.sch5, wave3m0.sch5, wave4m0.sch5, wave5m0.sch5),
                         lower = c(wave2m0.sch5 - wave2m0.sch5.se,
                                   wave3m0.sch5 - wave3m0.sch5.se,
                                   wave4m0.sch5 - wave4m0.sch5.se,
                                   wave5m0.sch5 - wave5m0.sch5.se),
                         upper = c(wave2m0.sch5 + wave2m0.sch5.se,
                                   wave3m0.sch5 + wave3m0.sch5.se,
                                   wave4m0.sch5 + wave4m0.sch5.se,
                                   wave5m0.sch5 + wave5m0.sch5.se))
m0.sch5.df

# school 6
wave2m0.sch6 <- summary(m0)$coefficient["tXw2",1] + summary(m0)$coefficient["factor(school)4",1]
wave3m0.sch6 <- summary(m0)$coefficient["tXw3",1] + summary(m0)$coefficient["factor(school)4",1]
wave4m0.sch6 <- summary(m0)$coefficient["tXw4",1] + summary(m0)$coefficient["factor(school)4",1]
wave5m0.sch6 <- summary(m0)$coefficient["tXw5",1] + summary(m0)$coefficient["factor(school)4",1]
wave2m0.sch6.se <- summary(m0)$coefficient["tXw2",2] + summary(m0)$coefficient["factor(school)6",2]
wave3m0.sch6.se <- summary(m0)$coefficient["tXw3",2] + summary(m0)$coefficient["factor(school)6",2]
wave4m0.sch6.se <- summary(m0)$coefficient["tXw4",2] + summary(m0)$coefficient["factor(school)6",2]
wave5m0.sch6.se <- summary(m0)$coefficient["tXw5",2] + summary(m0)$coefficient["factor(school)6",2]
m0.sch6.df <- data.frame(school = c(6, 6, 6, 6),
                         wave = seq(2:5),
                         estimate = c(wave2m0.sch6, wave3m0.sch6, wave4m0.sch6, wave5m0.sch6),
                         lower = c(wave2m0.sch6 - wave2m0.sch6.se,
                                   wave3m0.sch6 - wave3m0.sch6.se,
                                   wave4m0.sch6 - wave4m0.sch6.se,
                                   wave5m0.sch6 - wave5m0.sch6.se),
                         upper = c(wave2m0.sch6 + wave2m0.sch6.se,
                                   wave3m0.sch6 + wave3m0.sch6.se,
                                   wave4m0.sch6 + wave4m0.sch6.se,
                                   wave5m0.sch6 + wave5m0.sch6.se))
m0.sch6.df
```

```{r}
all.schools <- rbind(m0.sch1.df, m0.sch2.df, m0.sch3.df, m0.sch4.df, m0.sch5.df, m0.sch6.df)
saveRDS(all.schools, file = "../results/simulation/no_pooling_sim.RDS")
all.schools <- readRDS("../results/simulation/no_pooling_sim.RDS")
```
```{r}
m0_lsat_across_schools <-
  ggplot(data = all.schools, aes(x = wave, y = estimate)) +
  geom_pointrange(aes(ymin = lower, 
                      ymax = upper, group = school, color = school),
                  position=position_dodge(width=0.40)) +
  scale_x_discrete("Wave", limits=seq(2:6) )  +
  geom_hline(yintercept=0) + 
  theme(legend.position="bottom")
ggsave("../figures/no_pooling_lsat_across_schools.pdf", m0_lsat_across_schools, 
       width = 7, height = 4, units = "in")
m0_lsat_across_schools
```
```{r}
m01 <- plm(y ~ wave + treated + tXw2 + tXw3 + tXw4 + tXw5, data=d.p)
summary(m01)
wave2m01 <- summary(m01)$coefficient["tXw2",1]
wave3m01 <- summary(m01)$coefficient["tXw3",1]
wave4m01 <- summary(m01)$coefficient["tXw4",1]
wave5m01 <- summary(m01)$coefficient["tXw5",1]
wave2m01.se <- summary(m01)$coefficient["tXw2",2]
wave3m01.se <- summary(m01)$coefficient["tXw3",2]
wave4m01.se <- summary(m01)$coefficient["tXw4",2]
wave5m01.se <- summary(m01)$coefficient["tXw5",2]
m01.df <- data.frame(wave = seq(2:5),
                     complete = c(wave2m01, wave3m01, wave4m01, wave5m01),
                     lower = c(wave2m01 - wave2m01.se,
                               wave3m01 - wave3m01.se,
                               wave4m01 - wave4m01.se,
                               wave5m01 - wave5m01.se),
                     upper = c(wave2m01 + wave2m01.se,
                               wave3m01 + wave3m01.se,
                               wave4m01 + wave4m01.se,
                               wave5m01 + wave5m01.se))
saveRDS(m01.df, file = "../results/simulation/compl_pooling_sim.RDS")
m01.df <- readRDS("../results/simulation/compl_pooling_sim.RDS")
# complete.tibble <- tibble(school = c(7,7,7,7),
#                           wave = seq(2:5),
#                           estimate = m01.df$complete,
#                           lower = m01.df$lower,
#                           upper = m01.df$upper,
#                           Q10 = NA,
#                           Q90 = NA,
#                           method = "comppool")
```

```{r}
m01_lsat_across_schools_sim <- 
  ggplot(data = m01.df,
       aes(x = wave,
           y = complete)) +
  geom_pointrange(aes(ymin = lower,
                      ymax = upper),
                      position=position_dodge(width=0.50)) +
                    scale_x_discrete("Wave", limits = c(2,3,4,5)) +
                                       geom_hline(yintercept=0)
ggsave("../figures/compl_pooling_lsat_across_schools_sim.pdf", m01_lsat_across_schools_sim, 
       width = 7, height = 4, units = "in")
m01_lsat_across_schools_sim
```
```{r}
oneperiod <-  d %>% filter (wave==1 | wave==2) %>%
  select(wave, student, class, treated, tXw2, school, ustudent, student.effect, school.effect, y)

oneperiod %>%
  glimpse()
```
```{r}
sim1 <-brm(y ~ 0 + factor(school),
           prior = prior(normal(0, 10), class = b),
           cores = 4, chains = 4,
           data = oneperiod, seed = 123,
           file = "../results/simulation/sim1") 

sim2 <- brm(y ~ 1 + (1 | school),
            prior = c(prior(normal(0, 5), class = Intercept),
                      (prior(cauchy(0, 1), class = sd),
            cores = 4, chains = 4,
            sample_prior = "yes",
            data = oneperiod, seed = 123,
            file = "../results/simulation/sim2")
```

```{r}
# WAIC comparison

sim1 <- add_criterion(sim1, "waic")
sim2 <- add_criterion(sim2, "waic")

w <- loo_compare(sim1, sim2, criterion = "waic")
```


```{r}
cbind(waic_diff = w[, 1] * -2,
      se        = w[, 2] *  2)
```

```{r}
# WAIC weights

model_weights(sim1, sim2, weights = "waic") %>%
  round(digits = 2)
```

```{r}
w[, "p_waic"]
```

```{r}
print(sim2)
```

```{r}
post <- posterior_samples(sim2)

post_mdn <-
  coef(sim2, robust = T)$school[, , ] %>%
  data.frame() %>%
  bind_cols(d) %>%
  mutate(post_mdn = )
```




```{r}
prior <- c(prior(normal(0, 5), class = Intercept),
           prior(normal(0, 10), class = b), # prior population level/ population-level ('fixed') effects
           prior(cauchy(0, 1), class = sd)) # prior group-level
sim3 <- brm(y ~ treated + wave + tXw2 + (1 + treated + tXw2 | school) + (1 | ustudent),
           prior = prior, cores = 4, chains = 4, 
           data = oneperiod, seed = 123,
           file = "../results/simulation/sim3")
print(sim3)
```
Every population-level effect has its own regression parameter represents the name of the corresponding population-level effect. Suppose, tof instance, that `y` is predicted by `x1` and `x2` (i.e., `y ~ x1 + x2` in formula syntax). Then, `x1` and `x2` have regression parameters ``b_x1` and `b_x2` respectively. The dafault prior for population-level effects (including monotonic and category specific effects) is an improper flat prior over the reals. Other common options are normal priors or student-t priors. If we want to have a normal prior with mean `0` and standard deviation `5` for `x1`, and a unit sutdent-t prior with `10` degress of freedom for `x2`, we can specify this via `set_prior("nrmal(0,45), class = "b", coef = "x1"` and `set_prior("student_t(10,0,1), class = "b", coef = "x2"`. To put the same prior on all population-level effects at once, we may write as a shortcut `set_prior("<prior>", class = "b")`. This also leads to faster ampling, because priors can be vectorized in this case. Both ways of defining priors can be combined using for instance `set_prior("normal(0,2)", class = "b")` and `set_prior("normal(0,10)", class = "b", coef = "x1")` at the same time. This will se a `normal(0,10)`prior on the effect of `x1`and a `normal(0,2)` prior on all other population-level effects. However, this will break vectorization and may slow down the sampling procedure a bit.

Each group-level effect of each grouping factor has a standard deviation named `sd_<group>_<coef>`. Consider, for instance, the formulat `y ~ x1 + x2 + (1 + x1 | g)`. We see that the intercept s well as `x1` are goup-level effects nested in the grouping factor `g`. The corresponding standard deviation parameters are named as `sd_g_Intercept`and `sd_g_x1` respectively. These parameters are restricted to be non-negative and, by default, have a half student-t prior with 3 degrees fo freedom and a scale parameter that depends on the standard deviation of the response after applying the link funtion. Minimally, the scale parameter is 10. This prior is used (a) to be only very weakly informative in order to influence results as few as possible, whil (b) providing at least some regularization to considerably improve convergence and sampling efficiency. To define a prior distribution only for standard deviations of a specific grouping factor, use `set_prior("<prior>", class = "sd", group = "<group>")`. To define a prior distribution only for a specific standard deviation of a specific grouping factor, you may write `set_prior("<prior>", class = "sd", group = "<group>", coef = "<coef>")`. Recommendations on useful priors distributions for standard deviations are given in Gelman (2006), but note that he is no longer regommending uniform priors, anymore.

When defining priors on group-level parameters in non-linear models, please make sure to specify the corresponding non-linear parameter through `nlpar` argument in the same way as for population-level effects.


# Chapter 4 McElreath
$lsat_i \sim N(\mu, \sigma)$
$\mu \sim N(7,2)$
$\sigma \sim Exp(0,1)$
```{r}
# shape of the prior mu
p1 <- tibble(x = seq(from = 0, to = 10, by = 0.1)) %>%
  
  ggplot(aes(x = x, y = dnorm(x, mean = 7, sd = 2))) +
  geom_line() +
  scale_x_continuous(breaks = seq(from = 0, to = 10, by = 1)) +
  labs(title = "mu ~ dnorm(7, 2)", y = "density")
p1
```
```{r}
# shape of prior sigma
p2 <- tibble(x = seq(from = 0, to = 1, by = 0.1)) %>%
  
  ggplot(aes(x = x, y = dcauchy(seq(0,1,by=0.1)))) +
  geom_line() +
  scale_x_continuous(breaks = c(0, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("sigma ~ dcauchy(0, 1)")
p2
```
We can simulate from both priors at once to get a prior probability distribution of life satisfaction

```{r}
n <- 1e4

set.seed(4)

sim <-
  tibble(sample_mu = rnorm(n, mean = 7, sd = 2),
         sample_sigma = runif(n, 0, 1)) %>%
  mutate(lsat = rnorm(n, mean = sample_mu, sd = sample_sigma))
```

```{r}
p3 <- sim %>%
  ggplot(aes(x = lsat)) +
  geom_density(fill = "grey33") +
  xlim(0, 10) +
  scale_x_continuous(breaks = c(0, 5, 10)) +
  ggtitle("lsat ~ dnorm(mu, sigma)")
p3
```
```{r}
# computing the mean and three sd above and below
sim %>%
  summarise(ll = mean(lsat) - sd(lsat)*3,
            mean = mean(lsat),
            ul = mean(lsat) + sd(lsat)*3) %>%
  mutate_all(round, digits = 1)
```
```{r}
set.seed(4)

sim <-
  tibble(sample_mu = rnorm(n, mean = 7, sd = 2),
         sample_sigma =runif(n, min = 0, max = 1)) %>%
  mutate(lsat = rnorm(n, mean = sample_mu, sd = sample_sigma))

# Compute the values we'll use to break on our x axis
breaks <-
  c(mean(sim$lsat) - 3 * sd(sim$lsat), 0, mean (sim$lsat), mean(sim$lsat) + 3 * sd(sim$lsat)) %>%
  round(digits = 0)

# this is just for aesthetics
text <-
  tibble(height = 10 - 25,
         y = 0.0013,
         label = "max of scale",
         angle = 90)

# plot
p4 <-
  sim %>%
  ggplot(aes(x = lsat)) +
  geom_density(fill = "black", size = 0) +
  geom_vline(xintercept = 0, color = "grey92") +
  geom_vline(xintercept = 10, color = "grey92", linetype = 3) +
  # geom_text(data = text,
  #           aes(y = y, label = label, angle = angle),
  #           color = "grey92") +
  xlim(0, 10) +
  scale_x_continuous(breaks = breaks) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("lsat ~ dnorm(mu, sigma)\nmu ~ dnorm(7,2)") +
  theme(panel.grid = element_blank())

p4
```














# Original computing environment

```{r}
devtools::session_info()
```

